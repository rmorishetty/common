#!./venv/bin/python

# wf_generator.py
# an XML document builder for oozie


import csv
import hdfs
import sys
import imp
import datetime
import getpass
from datetime import datetime
from datetime import timedelta
import time
sys.path.append("./")
import workflow
import woozie
import darn
import api
import ast

# this magical construct to overcome python failing
# to understand symlinks generated by distributed cache
# as importable modules
#sys.path.append("./")
#sys.path.append("./hapi")
#workflow = imp.load_source('workflow', './hapi/workflow.py')
#woozie = imp.load_source('woozie', './hapi/woozie.py')

start_date = sys.argv[1]
end_date = sys.argv[2]
src = sys.argv[3]
WEBHDFS = sys.argv[4]
WORKFLOW_DIR = sys.argv[5]
CLEAN_SCRIPT_DIR = sys.argv[6]
CLEAN_DATA_DIR = sys.argv[7]
INDICATOR_DIR = sys.argv[8]
OZ_TOOL_DIR = sys.argv[9]
TOOL_DIR = sys.argv[10]
INDICATOR_EXE = sys.argv[11]
INDICATOR_WF = sys.argv[12]
NAME_NODE_DEFAULT = sys.argv[13]
JOB_TRACKER_DEFAULT = sys.argv[14]
USE_SYSTEM_LIBPATH_DEFAULT = sys.argv[15]
SHARE_LIBPATH_DEFAULT = sys.argv[16]
OOZIE_JOBS_ENDPOINT = sys.argv[17]
SCHEDULER_ENDPOINT = sys.argv[18]
METRICS_ENDPOINT = sys.argv[19]
USER = sys.argv[20]
QUEUE_NAME = sys.argv[21]
GLOBAL_CONF_LIST = sys.argv[22]
SQOOP_OPT_DIR = sys.argv[23]
MYSQL_LIB = sys.argv[24]
WRAPPER_SCRIPT = sys.argv[25]
CONF_DIR = sys.argv[26]
CLEAN_SCRIPT = sys.argv[27]

yesterday = datetime.strptime(start_date, '%Y%m%d') - timedelta(days=1)
execution_date = yesterday.strftime('%Y%m%d')

start_date = execution_date
end_date = execution_date

# config constants
CSV_CONFIG_FILE = './table_list.csv'

#yesterday = datetime.date.today() - datetime.timedelta(days=1)
#execution_date = yesterday.strftime('%Y%m%d')

# get control file
with open(CSV_CONFIG_FILE,'r') as file:
    reader =  csv.reader(file)
    headers = reader.next()
    control = []
    for row in reader:
        control.append(dict(zip(headers, row[0:])))
    print control


# iterate control file to build workflow and submit job
with woozie.OozieManager(user=USER, queue_name=QUEUE_NAME, OOZIE_JOBS_ENDPOINT=OOZIE_JOBS_ENDPOINT, SCHEDULER_ENDPOINT=SCHEDULER_ENDPOINT, METRICS_ENDPOINT=METRICS_ENDPOINT) as m:
    i = 1
    for item in control:
        table = item['source_table']
        target_dir = item['target_dir']
        target_prefix = item['prefix']
        target_table = item['target_table']
        options = item['options']
        mappers = item['mappers']
        source = item['source']

        print table, target_dir,target_prefix,target_table, options, mappers
        # build workflow
        workflow_name = 'wf_%d_%s_%s' % (i, target_prefix, target_table)
        workflow_name = workflow_name[:50]
        #conf_list = []
        #global_conf_list = [('oozie.launcher.yarn.app.mapreduce.am.resource.mb','8192'), ('oozie.launcher.mapred.job.queue.name','data_products'),('mapred.job.queue.name','prod')]
        global_conf_list = ast.literal_eval(GLOBAL_CONF_LIST)
        with workflow.Workflow(name=workflow_name, conf_list=global_conf_list) as wf:
            with workflow.FSAction() as a:
                a.delete(target_dir)
                wf.insert_action(a)

            with workflow.SqoopAction() as a:
                #inp_list = [('oozie.launcher.mapreduce.map.memory.mb','1536')]
                #a.configsec(inp_list)
                a.arg('import')
                a.arg('-Dorg.apache.sqoop.splitter.allow_text_splitter=true')
                a.arg('--driver=com.mysql.jdbc.Driver')
                a.arg('--table=' + table)
                a.arg('--target-dir=' + target_dir)
                a.arg('--options-file')
                a.arg(options)
                a.arg('--verbose')
                a.arg('-m=' + str(mappers))
                a.file(SQOOP_OPT_DIR + options + '#' + options)
                a.file(MYSQL_LIB + '#mysql-connector-java.jar')
                wf.insert_action(a)

            with workflow.FSAction() as a:
                a.chmod(target_dir, '775')
                wf.insert_action(a)

            #with workflow.SubWorkflowAction() as a:
                #inp_list = [('db','raw'),('job_status','success'),('start_date',start_date),('end_date',end_date),('source',target_prefix),('subject',target_table),('clean_db_dir',CLEAN_DATA_DIR),('audit_ind_dir',INDICATOR_DIR),('nameNode',NAME_NODE_DEFAULT), ('dp_tool_dir',TOOL_DIR),('dp_script_dir',TOOL_DIR)]
                #inp_list.extend(global_conf_list)
                #a.app_path(OZ_TOOL_DIR + '/' + INDICATOR_WF)
                #a.configsec(inp_list)
                #wf.insert_action(a)

            with workflow.FSAction() as a:
                ind_dir = INDICATOR_DIR + '/' + 'datestamp=' + start_date
                ind_file = ind_dir + '/' + 'raw.' + target_prefix + '_' + target_table + '.success'
                a.mkdir(ind_dir)
                a.touchz(ind_file)
                #a.chmod(ind_dir, '775')
                wf.insert_action(a)

            #with workflow.PigAction() as a:
                #a.script(CLEAN_SCRIPT_DIR + '/' + 'clean_' + target_prefix + '_' + target_table + '.pig')
                #a.param('TARGET_FILE=' + CLEAN_DATA_DIR + source + '/' + target_prefix + '/' + target_table)
                #wf.insert_action(a)

            #with workflow.FSAction() as a:
            #    a.touchz(INDICATOR_DIR + execution_date + '.clean_' + source + '_' + target_prefix + '_' + target_table + '.success')
            #    wf.insert_action(a)

            #with workflow.ShellAction() as a:
                #a.execute(INDICATOR_EXE)
                #a.argument(source)
                #a.argument(target_prefix + '_' + target_table)
                #a.argument('clean')
                #a.argument('success')
                #a.argument(start_date)
                #a.argument(end_date)
                #a.argument(CLEAN_DATA_DIR)
                #a.env_var('HDFS_server=${nameNode}')
                #a.env_var('HDFS_audit=' + INDICATOR_DIR)
                #a.file(TOOL_DIR + '/' + INDICATOR_EXE + '#' + INDICATOR_EXE)
                #wf.insert_action(a)

            with workflow.ShellAction() as a:
                a.execute(WRAPPER_SCRIPT)
                a.argument(CLEAN_SCRIPT)
                a.argument(start_date)
                a.argument(target_prefix + '_' + target_table + '.cfg')
                a.env_var('HADOOP_USER_NAME=${wf:user()}')
                a.file(CLEAN_SCRIPT_DIR + '/' + WRAPPER_SCRIPT + '#' + WRAPPER_SCRIPT)
                a.file(CLEAN_SCRIPT_DIR + '/' + CLEAN_SCRIPT + '#' + CLEAN_SCRIPT)
                a.file(CONF_DIR + '/' + target_prefix + '_' + target_table + '.cfg' + '#' + target_prefix + '_' + target_table + '.cfg')
                a.capture_out()
                wf.insert_action(a)

            #with workflow.SubWorkflowAction() as a:
                #inp_list = [('db','clean'),('job_status','success'),('start_date',start_date),('end_date',end_date),('source',target_prefix),('subject',target_table),('clean_db_dir',CLEAN_DATA_DIR),('audit_ind_dir',INDICATOR_DIR),('nameNode',NAME_NODE_DEFAULT), ('dp_tool_dir',TOOL_DIR),('dp_script_dir',TOOL_DIR)]
                #inp_list.extend(global_conf_list)
                #a.app_path(OZ_TOOL_DIR + '/' + INDICATOR_WF)
                #a.configsec(inp_list)
                #wf.insert_action(a)

            with workflow.FSAction() as a:
                ind_dir = INDICATOR_DIR + '/' + 'datestamp=' + start_date
                ind_file = ind_dir + '/' + 'clean.' + target_prefix + '_' + target_table + '.success'
                a.mkdir(ind_dir)
                a.touchz(ind_file)
                #a.chmod(ind_dir, '775')
                wf.insert_action(a)

            # write workflow to hdfs
            c = hdfs.client.InsecureClient(WEBHDFS, USER)
            path = '%s/%s_%s.xml' % (WORKFLOW_DIR, target_prefix, target_table)
            c.write(hdfs_path=path, data=wf.to_string(), overwrite=True, permission='775')

        # build workflow configuration
        with workflow.Configuration(NAME_NODE_DEFAULT, JOB_TRACKER_DEFAULT, USE_SYSTEM_LIBPATH_DEFAULT, SHARE_LIBPATH_DEFAULT) as cf:
            cf.property(key='oozie.wf.application.path', value=path)
            cf.property(key='user.name', value=USER)
            cf.property(key='oozie.action.sharelib.for.pig', value='pig,hcatalog,hive')
            cf.property(key='oozie.launcher.mapreduce.map.memory.mb', value='2048')
            configuration = cf.to_string()
            print configuration

        # submit job to manager queue
        m.submit_job(configuration, mappers)
        i += 1
    # run the manager queue
    m.run_queue()

# make success file
#c = hdfs.client.InsecureClient(WEBHDFS, USER)
#path = '/auction/audit/process_indicator/%s/workflow_end/import_mlh_rcp_transactional_data/_SUCCESS' % execution_date
#c.write(hdfs_path=path, data='', overwrite=True, permission='775')
